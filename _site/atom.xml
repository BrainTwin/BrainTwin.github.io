<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://localhost:4000/atom.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-07-13T17:27:16+09:00</updated><id>http://localhost:4000/atom.xml</id><title type="html">BrainTwin</title><subtitle>About Artificial Intelligence...</subtitle><author><name>WooSang Shin</name></author><entry><title type="html">UnUnlearning: Unlearning is not sufficient for content regulation in advanced generative AI for algebra</title><link href="http://localhost:4000/paper%20review/paper-review-ununlearning/" rel="alternate" type="text/html" title="UnUnlearning: Unlearning is not sufficient for content regulation in advanced generative AI for algebra" /><published>2024-07-11T00:00:00+09:00</published><updated>2024-07-11T00:00:00+09:00</updated><id>http://localhost:4000/paper%20review/paper-review-ununlearning</id><content type="html" xml:base="http://localhost:4000/paper%20review/paper-review-ununlearning/"><![CDATA[<p>본격적인 연구 내용을 알아보기에 앞서 이 글에서 다룰 내용이 왜 심각하게 논의되어야 하는지 살펴볼 필요가 있다. 아마 GPT, Claude, Gemini와 같은 모델들을 사용해봤다면 소위 Large Language Model(LLM) 이라고 불리는 놈들의 엄청난 지적능력에 분명 한번쯤은 놀란 경험을 가지고 있을 것이다. 추상적인 질문에도 기가막힌 대답을 내어놓는다. 심지어 문장 구성이나 맥락도 완벽에 가까울정도로 체계적이고 구조가 잘 갖춰져있다. 하지만 때로는 인간(?)적인 면모를 보여주기도 한다. 매우 단순한 연산임에도 틀린 답을 말하면서 심지어 뚝심있게 어떠한 회유나 압박에도 굴하지 않고 동일한 답변을 주기도 한다.</p>

<section class="link-in-post">
  
  
  

<article class="entry h-entry">
  <header class="entry-header">
    <h3 class="entry-title p-name">
      
        <a href="/paper%20review/paper-review-tree-rings-watermarks-invisible-fingerprints/" rel="bookmark">Tree-Rings Watermarks: Invisible Fingerprints for Diffusion Images
</a>
      
    </h3>
    
      
      
      
      <img class="entry-image u-photo" src="/images/2024-07-10-paper-review-tree-rings-watermarks-invisible-fingerprints/thumbnail.png" alt="" />
    
  </header>
  
  
    <footer class="entry-meta">
      
      <time class="entry-date dt-published" datetime="2024-07-10T00:00:00+09:00">July 10, 2024
  </time>
    </footer>
  
</article>

</section>

<h3 id="terminology">Terminology</h3>
<ul>
  <li>Impermissible knowledge : 비허가 지식 or 정보</li>
  <li>Unlearning :</li>
  <li>Content regulation :</li>
  <li>Biological(or nuclear) knowledge :</li>
  <li>Contextual interaction :</li>
  <li>Resurgence of undesirable knowledge :</li>
  <li>In-context learning :</li>
</ul>

<h3 id="main-argument">Main Argument</h3>

<p>이 논문에서 저자들은 기존 unlearning schemes의 feasibility를 논의하고자함. 엄밀하게는 model이 가진 특정 정보의 규제(specific information regulation)가 unlearning으로 가능한지 의혹을 제기.</p>

<p>In-context learning 관점에서 특정 word는 다른 정보들의 조합으로 표현.</p>

<p><img src="/images/2024-07-11-paper-review-ununlearning/0.png" alt="knowledge" /></p>

<p>이 관점에서 보면 관련 정보들을 모두 제거하는 것이 아니라면 특정 정보를 unlearning하더라도 우회적인 방법을 통해 충분히 정보를 유추해낼 수 있음. 이는 unlearning의 필요성에 바탕이 되는 비허가 정보의 유출을 막는 것이 기존 전략으로는 어렵다는 것을 시사.</p>

<p><img src="/images/2024-07-11-paper-review-ununlearning/1.png" alt="example" /></p>

<p>(예 :<code class="language-plaintext highlighter-rouge">폭탄</code> 제조 방법에 대한 정보를 모델이 가지고 있을때 <code class="language-plaintext highlighter-rouge">폭탄</code>에 대한 정보를 제거(unlearning)하더라도 <code class="language-plaintext highlighter-rouge">폭탄</code> 의 속성을 부여한 새로운 단어를 정의함으로써, 제조 방법에 대한 정보를 접근 할 수 있게 됨.)</p>

<h3 id="nomenclature">Nomenclature</h3>

<p>저자들은 비공식적으로 다음 개념들을 정의함.</p>

<p>Knowledge : 모델이 이용가능한 모든 정보. 입력에 의해 파생될 수 있는 in-contextual 정보, model의 parameter에 저장된 정보, retrieval에 이용가능한 evidence들을 모두 포함.</p>

<p>Content filtering :</p>

<p>Unlearning :</p>

<p>Unlearning for privacy :</p>

<p>Unlearning for content regulation :</p>

<p>In-Context learning :</p>

<h3 id="types-of-knowledge">Types of knowledge</h3>

<p>Axioms</p>

<p>Theorems</p>]]></content><author><name>WooSang Shin</name></author><category term="Paper Review" /><category term="Unlearning" /><category term="Knowledge Regulation" /><summary type="html"><![CDATA[본격적인 연구 내용을 알아보기에 앞서 이 글에서 다룰 내용이 왜 심각하게 논의되어야 하는지 살펴볼 필요가 있다. 아마 GPT, Claude, Gemini와 같은 모델들을 사용해봤다면 소위 Large Language Model(LLM) 이라고 불리는 놈들의 엄청난 지적능력에 분명 한번쯤은 놀란 경험을 가지고 있을 것이다. 추상적인 질문에도 기가막힌 대답을 내어놓는다. 심지어 문장 구성이나 맥락도 완벽에 가까울정도로 체계적이고 구조가 잘 갖춰져있다. 하지만 때로는 인간(?)적인 면모를 보여주기도 한다. 매우 단순한 연산임에도 틀린 답을 말하면서 심지어 뚝심있게 어떠한 회유나 압박에도 굴하지 않고 동일한 답변을 주기도 한다.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/images/main.png" /><media:content medium="image" url="http://localhost:4000/images/main.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Tree-Rings Watermarks: Invisible Fingerprints for Diffusion Images</title><link href="http://localhost:4000/paper%20review/paper-review-tree-rings-watermarks-invisible-fingerprints/" rel="alternate" type="text/html" title="Tree-Rings Watermarks: Invisible Fingerprints for Diffusion Images" /><published>2024-07-10T00:00:00+09:00</published><updated>2024-07-10T00:00:00+09:00</updated><id>http://localhost:4000/paper%20review/paper-review-tree-rings-watermarks-invisible-fingerprints</id><content type="html" xml:base="http://localhost:4000/paper%20review/paper-review-tree-rings-watermarks-invisible-fingerprints/"><![CDATA[<p><a href="https://openreview.net/pdf?id=Z57JrmubNl">openreview.net</a></p>

<h3 id="diffusion-inversion">Diffusion Inversion</h3>

<p>: Reverse diffusion process의 initial state $x_T$를 $x_0$로 부터 sampling하는 것.</p>

<p>(Reverse process와 Forward process가 서로 adjoint 되어 있음이 보장되면 가능.)</p>

<p>Learned reverse transition probabilistic model $\epsilon_\theta(x_t)$을 이용하여 opposite direction으로 이동할 수 있음.</p>

<p>$\hat x_0=D_\theta(x_T,0)\approx x_0$</p>

<p>Here, $D_\theta$ is reverse diffusion process.</p>

<p>This mechanism depends on the assumption that  $x_{t+1}-x_t\approx x_{t}-x_{t-1}$ (논문에 표기된 수식은 오타인 듯)</p>

\[x_{t+1} = \sqrt{\bar \alpha_{t+1}}\hat x_0^t + \sqrt{1-\bar \alpha_{t+1}}\epsilon_\theta(x_t).\]

<p>$D^\dagger_\theta$ denotes inversion process by above equation.</p>

<p>정리하자면, $\hat x_T=D^\dagger_\theta(x_0)\approx x_T$</p>

<h3 id="tree-ring-watermarking">Tree-Ring Watermarking</h3>

<p>: Choosing the initial noise state so that its Fourier transform contains a carefully constructed pattern near its center. (Center = Low frequency region) This pattern is called as ‘key’.</p>

<p>바로 $x_T$에 key를 삽입하면 결과 이미지에 알아챌만한 패턴이 나타날 수 있음.</p>

<p>Fourier space를 이용하는 이유는 invariant property들이 잘 알려져 있음.</p>]]></content><author><name>WooSang Shin</name></author><category term="Paper Review" /><category term="Signature" /><category term="Authentication" /><category term="Authorization" /><summary type="html"><![CDATA[openreview.net]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/images/main.png" /><media:content medium="image" url="http://localhost:4000/images/main.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>